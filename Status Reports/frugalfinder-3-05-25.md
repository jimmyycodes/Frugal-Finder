# Weekly Status Report (03.05.25)

## Project Status

### Goals for the Week
- (Copy from "Goals for next week" in last week's report. Leave empty for the first week.)

### Progress and Issues
- Tasks completed:
  - Backend & Scaper Core elements
- Key learnings:
  - Real world SWE talk
- Issues encountered:
  - [Issue 1] and steps taken to resolve it
  - [Issue 2] (if unresolved, include blockers and additional context)

### Goals for Next Week
- FINSIH!!!

---

### David Lym

### Goals for the Week
- Complete end to end setup

### Progress and Issues
- Tasks completed:
  - Connecting pages
  - Backend and frontend coordination
- Key learnings:
  - Talk and coordinate as much as possible. 
- Issues encountered:
  - Time completion before final release is close.
  - Dependencies are messeing up sometimes & a couple problems with setup instructions.

### Goals for Next Week
- Backend completion (AWS or Azure)(Whole Team) - Estimated time: [5 hours]
- Frontend cleaned up (Nathan David) - Estimated time: [3 hours]

---

## [Nathan Moreland]

### Goals for the Week
- Have UI ready to show for Beta Test

### Progress and Issues
- Tasks completed:
  - Several UI components
  - Working Android Emulator
- Key learnings:
  - Using React Native
- Issues encountered:
  - Time until Beta is tight

### Goals for Next Week
- Start on getting the backend connected
- Get the database and the webscrapper connected

---

## [Jimmy Le]

### Goals for the Week
- Finish up scrapers and automating the script to run it daily

### Progress and Issues
- Tasks completed:
  - Finished building abstract structure for scraper.
  - Finsihed connecting scraper to DB
  - Finished setting up CRON to run scraper daily.
- Key learnings:
  - I learned that Github has secret actions you use to set up environments for the workflow when running certain things.
  - I also learned how niche and specific git hub is when it comes to running scraping files as it needs to specifically download specific chrome/chromedrivers and specific dependencies.
  - I learned how to debug error codes given from github actions as they are very very vague.
- Issues encountered:
  - Issues with connecting environment secrets to github workflow
  - Issues with setting up proper chromedrivers to run scrapers automatically through CRON jobs. 

### Goals for Next Week
- Help set up google maps api and comparison algorithm [3 hours]
- Catch up on front end and back end work [3 hours]
- Potentially build additional scraper for walmart [1-2 hours]
---

## [Di Ramirez-Diaz]

### Goals for the Week
- (Copy from "Goals for next week" in last week's report. Leave empty for the first week.)

### Progress and Issues
- Tasks completed:
  - [Task 1]
  - [Task 2]
- Key learnings:
  - [What was learned this week]
- Issues encountered:
  - [Issue 1] and steps taken to resolve it
  - [Issue 2] (if unresolved, include blockers and additional context)

### Goals for Next Week
- [Task 1] (Responsibility: [Team Member 1]) - Estimated time: [X hours]
- [Task 2] (Responsibility: [Team Member 2]) - Estimated time: [X hours]
- [Task 3] Longer-term goal (Responsibility: [Team Member or Team])

---

## [Vladyslav Boiko]

### Goals for the Week
- Finally finish with database setup, connect to Azure or move on to SQLLite.
- Present Beta release.

### Progress and Issues
- Tasks completed:
  - Created users for AWS DB 
  - Created server and DB in AWS
  - Since we weren't able to connect through VScode, removed everything from AWS
- Key learnings:
  - AWS is extremely hard to connect to.
- Issues encountered:
  - AWS is so complecated, cannot find a single tutorial on how to grand IAM to other users 
    to connect through VScode extension.

### Goals for Next Week
- Connect scraper to DB.
- Connect DB to backend of the app.

---

## [Ronin Crawford]

### Goals for the Week
- Work more on setting up AWS: [4 hours]
- Help set up a basic back-end to prepare for the beta release: [4 hours]

### Progress and Issues
- Tasks completed:
  - Helped create MySQL database using AWS
  - Worked on connecting AWS database to VSCode, but still did not manage to get it to work (temporarily switching to Azure)
- Key learnings:
  - I learned a lot about how a large-scale cloud service like AWS functions, particularly how complicated it is to use
  - I learned more about how the web scraper works from Jimmy, potential problems we could run into
- Issues encountered:
  - Still stuck with AWS, switching to Microsoft Azure temporarily because we have used it before to be ready for the beta test
  - Did not end up helping set up the back-end

### Goals for Next Week
- Work on optimizing our Azure database, integrating it effectively with the front-end and scraper: [5 hours]

---
